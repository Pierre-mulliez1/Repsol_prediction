{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PIERRE~1\\AppData\\Local\\Temp/ipykernel_9204/3785050460.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import the necessary packages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\Anaconda\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdependency\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhard_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{dependency}: {e}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\Anaconda\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfft\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpolynomial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mctypeslib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\Anaconda\\lib\\site-packages\\numpy\\random\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\Anaconda\\lib\\site-packages\\numpy\\random\\_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Python\\Anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import the necessary packages \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "import re\n",
    "from datetime import datetime as dttime\n",
    "from datetime import date\n",
    "import calendar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif,SelectKBest\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from dateutil.relativedelta import *\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import BallTree\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div ><img src=\"Repsol-Logo.png\" width=300 height=200  style=\"border-radius:5%\"></div>\n",
    "\n",
    "# Fuel Demand & Pricing - Repsol\n",
    "\n",
    "**Description:** Data competition -- Repsol Fuel pricing\n",
    "\n",
    "**Time:** 3 Days \n",
    " \n",
    "**Client:** Repsol \n",
    "\n",
    "**Code created:** 2021-03-15 <br>\n",
    "\n",
    "**Last updated:** 2021-03-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "We have two main datasets:\n",
    "\n",
    "**DATASET_REP**\n",
    "The dataset provided of Repsol data contains the following timeseries with daily granularity.\n",
    "- Date: Reference date %m/%d/%y (2016-2019)\n",
    "- Code: Code of Provice of Spain (PoS)\n",
    "- Province: Name of the province (\"A Coru√±a\")\n",
    "- Longitude: Location of the POS\n",
    "- Latitute: Location of the POS\n",
    "- Red: Supplier/Brand\n",
    "- Gasoline Demand: Normalized (0-1) volume sold\n",
    "- Gasoline Price: Gasoline pump price\n",
    "- Diesel Demand: Normalized (0-1) volume sold\n",
    "- Diesel: Diesel pump price\n",
    "\n",
    "**DATASET_COMP**\n",
    "- Similar to the other dataset but for the POS of competitors and only containing prices (not demand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and inspect the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dataset\n",
    "df = pd.read_excel('Dataset_REP.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning df to another var / avoid reloading the dataset if an eroor arises \n",
    "df1 = df.copy()\n",
    "name_graph = \"Fuel prices\"\n",
    "target = \"Diesel price\"\n",
    "target_2 = \"Gasoline price\"\n",
    "gd = \"Gasoline demand\"\n",
    "dd = \"Diesel demand\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Inspection\n",
    "## Understanding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datetime to unix for performance\n",
    "import datetime\n",
    "df1['Date']  = df1['Date'].astype('datetime64[ns]')\n",
    "df1['dateunix'] = (df1['Date'].apply(lambda x: x.toordinal()) - datetime.date(1970, 1, 1).toordinal()) * 24*60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.head())\n",
    "#grouping price and demand for visualization\n",
    "df2 = df1.groupby( ['Date']).mean()\n",
    "df2.reset_index(inplace=True)\n",
    "print(\"\")\n",
    "print(\"Size of grouped dataset: \" + str(len(df2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe qualitative features of the dataset\n",
    "df.describe(include=[object]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe quantitative features of the dataset\n",
    "df.describe(exclude=[np.object]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the dataframe\n",
    "size = df1.shape\n",
    "print(\"Size of the Dataframe -> {}\".format(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column data types\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas-profiling\n",
    "\n",
    "#from pandas_profiling import ProfileReport\n",
    "#report = ProfileReport(df1, minimal=False)\n",
    "#report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.RandomState(0)\n",
    "corr = df1.corr()\n",
    "corr.style.background_gradient(cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot \n",
    "#size of rba\n",
    "print(len(df1[dd]))\n",
    "#number of nulls\n",
    "print(df1[dd].isnull().sum())\n",
    "#other stat\n",
    "print(df1[dd].describe())\n",
    "\n",
    "\n",
    "bins = 10\n",
    "rbahist = df1[dd].hist(bins = bins, figsize = (10, 10),\n",
    "                                   grid = False,facecolor='r',range=(0,1))\n",
    "#set x axis legend to the median of the bin\n",
    "plt.xticks(rotation=90,fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "rbahist.set_xlabel(dd,fontsize=18)\n",
    "rbahist.set_ylabel(('Frequency') ,fontsize=20)\n",
    "rbahist.set_title('Distribution of  ' + dd,fontsize=20,weight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"skeweness test \")\n",
    "data = pd.to_numeric(df1[dd], downcast='integer')\n",
    "print(str(scipy.stats.skew(data)) + \", we have a normally dirtibutted target variable!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot \n",
    "#size of rba\n",
    "print(len(df1[gd]))\n",
    "#number of nulls\n",
    "print(df1[gd].isnull().sum())\n",
    "#other stat\n",
    "print(df1[gd].describe())\n",
    "\n",
    "\n",
    "bins = 10\n",
    "rbahist = df1[gd].hist(bins = bins, figsize = (10, 10),\n",
    "                                   grid = False,facecolor='r',range=(0, 1))\n",
    "#set x axis legend to the median of the bin\n",
    "plt.xticks(rotation=90,fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "rbahist.set_xlabel(gd,fontsize=18)\n",
    "rbahist.set_ylabel(('Frequency') ,fontsize=20)\n",
    "rbahist.set_title('Distribution of  ' + gd,fontsize=20,weight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"skeweness test \")\n",
    "data = pd.to_numeric(df1[gd], downcast='integer')\n",
    "print(str(scipy.stats.skew(data)) + \", we have a normally dirtibutted target variable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gasoline and Diesel demand over time \n",
    "x = df2['Date']\n",
    "y = df2[gd]\n",
    "y2 = df2[dd]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "plt.plot(x, y, color= \"r\",label = gd,lw = 0.3)\n",
    "plt.plot(x, y2, color= \"b\",label = dd,lw = 0.3)\n",
    "plt.xlabel(\"Gasoline vs Diesel\",fontsize = 16)\n",
    "plt.ylabel('Demand',fontsize = 16)\n",
    "plt.legend(bbox_to_anchor=(1.3, 0.80), loc='center', borderaxespad=0., fontsize = 15)\n",
    "plt.title(\"Demand evolution\",fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gasoline / Diesel price over time \n",
    "x = df2['Date']\n",
    "y = df2[target]\n",
    "y2 = df2[target_2]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "plt.plot(x, y, color= \"y\",label = target,lw = 0.5)\n",
    "plt.plot(x, y2, color= \"g\",label = target_2,lw = 0.5)\n",
    "plt.xlabel(\"Gasoline vs Diesel\",fontsize = 16)\n",
    "plt.ylabel('Price',fontsize = 16)\n",
    "plt.legend(bbox_to_anchor=(1.3, 0.80), loc='center', borderaxespad=0., fontsize = 15)\n",
    "plt.title(\"Price Evolution\",fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspecting elasticity \n",
    "#Gasoline\n",
    "x = df2[gd]\n",
    "y = df2[target_2]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "plt.scatter(x, y, color= \"r\",label = None,linewidths = 0.5)\n",
    "plt.xlabel(gd,fontsize = 16)\n",
    "plt.ylabel('Price',fontsize = 16)\n",
    "plt.title(gd + \" price elasticity\",fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diesel \n",
    "x = df2[dd]\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "plt.scatter(x, y, color= \"r\",label = None,linewidths = 0.5)\n",
    "plt.xlabel(dd,fontsize = 16)\n",
    "plt.ylabel('Price',fontsize = 16)\n",
    "plt.title(dd + \" price elasticity\",fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#region capital \n",
    "#la coruna, santiago de compostela and ferrol respectively to their indexes in the array\n",
    "x2 = [43.3623,42.8782,43.4896]\n",
    "y2 = [-8.396,-8.5448,-8.2193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using longitude and latitude \n",
    "#color scale by gasoline price\n",
    "x = df1['Latitude']\n",
    "y= df1['Longitude']\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "plt.scatter(x, y,label = \"station locations\",linewidths = 1, c= df1[target_2],s= 400 ,cmap=\"brg\")\n",
    "plt.scatter(x2, y2, color= \"b\",label = \"large cities locations\",linewidths = 1,marker = 'D',s=100)\n",
    "plt.xlabel(\"Latitude\",fontsize = 16)\n",
    "plt.ylabel('Longitude',fontsize = 16)\n",
    "plt.legend(bbox_to_anchor=(1.3, 0.80), loc='center', borderaxespad=0., fontsize = 15)\n",
    "plt.title( \" Location clusters\",fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using longitude and latitude \n",
    "#color scale by gasoline demand\n",
    "x = df1['Latitude']\n",
    "y= df1['Longitude']\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "plt.scatter(x, y,label = \"station locations\",linewidths = 1, c= df1[gd],s= 400 ,cmap=\"brg\")\n",
    "plt.scatter(x2, y2, color= \"b\",label = \"large cities locations\",linewidths = 1,marker = 'D',s=100)\n",
    "plt.xlabel(\"Latitude\",fontsize = 16)\n",
    "plt.ylabel('Longitude',fontsize = 16)\n",
    "plt.legend(bbox_to_anchor=(1.3, 0.80), loc='center', borderaxespad=0., fontsize = 15)\n",
    "plt.title( \" Location clusters\",fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Date'].dt.year.apply(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating variables from the original ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Month'] = df1['Date'].dt.month.apply(int)\n",
    "df1['Weekday'] = df1['Date'].dt.weekday.apply(int)\n",
    "df1['Day'] = df1['Date'].dt.day.apply(int)\n",
    "df1['Year'] = df1['Date'].dt.year.apply(int)\n",
    "#Grouped dataset date \n",
    "df2['Month'] = df2['Date'].dt.month.apply(int)\n",
    "df2['Weekday'] = df2['Date'].dt.weekday.apply(int)\n",
    "df2['Day'] = df2['Date'].dt.day.apply(int)\n",
    "df2['Year'] = df2['Date'].dt.year.apply(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding external variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding dataset to consider weather\n",
    "weather = pd.read_csv(\"New Temp.csv\", sep = \";\")\n",
    "weather.drop_duplicates(subset='FECHA').reset_index(drop=True)\n",
    "weather = weather.rename(columns={\"FECHA\": \"Date\"})\n",
    "weather['Date']  = weather['Date'].astype('datetime64[ns]')\n",
    "df1 = df1.merge(weather, on='Date', how='left' )\n",
    "print(\"Source: http://www.aemet.es/en/datos_abiertos/AEMET_OpenData\")\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brent Index data\n",
    "crude = pd.read_csv(\"Brent_Crude.csv\")\n",
    "crude['Date']  = crude['Date'].astype('datetime64[ns]')\n",
    "df1 = df1.merge(crude, on='Date', how='left' )\n",
    "print(\"Source: https://es.investing.com/commodities/brent-oil-historical-datab\")\n",
    "crude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding feature for holidays\n",
    "!pip install holidays-es\n",
    "\n",
    "from holidays_es import Province\n",
    "holi = pd.DataFrame.from_dict(Province(name=\"la-coruna\", year=2016).holidays(), orient='index').T\n",
    "for i in range (2017,2020):\n",
    "    holidays = Province(name=\"la-coruna\", year=i).holidays()\n",
    "    hello = pd.DataFrame.from_dict(holidays, orient='index').T\n",
    "    holi = pd.concat([holi, hello], axis=1)\n",
    "    \n",
    "holi = pd.concat([holi, holi.T.stack().reset_index(name='new')['new']], axis=1)\n",
    "holi = pd.DataFrame(holi.iloc[:,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holi[\"holiday\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding data from holidays df to datetime and merging with main dataframe\n",
    "holi[\"Date\"] = pd.to_datetime(holi[\"new\"])\n",
    "df1 = pd.merge(df1,holi, how = 'left',  on=\"Date\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(columns=[\"new\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['holiday'] = df1['holiday'].replace(np.nan, '0')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding feature to consider season\n",
    "def season(x):\n",
    "    if x == 12 or x == 1 or x == 2:\n",
    "        return 'Winter'\n",
    "    elif x == 3 or x == 4 or x == 5:\n",
    "        return 'Spring'\n",
    "    elif x == 6 or x == 7 or x== 8:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "    \n",
    "df1['season'] = df1['Month'].apply(season)\n",
    "df2['season'] = df2['Month'].apply(season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding feature for sunday because trucks aren't allowed to transit on this day \n",
    "def sunday(x):\n",
    "    if x == 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df1['Sunday'] = df1['Weekday'].apply(sunday)\n",
    "df2['Sunday'] = df2['Weekday'].apply(sunday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# code to create the geopoint\n",
    "!pip install geopandas\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "gdf = geopandas.GeoDataFrame(\n",
    "    df1, geometry=geopandas.points_from_xy(df1.Longitude, df1.Latitude))\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO FIND THE NN (nearest neighbour)\n",
    "\n",
    "# def get_nearest(src_points, candidates, k_neighbors=1):\n",
    "#     \"\"\"Find nearest neighbors for all source points from a set of candidate points\"\"\"\n",
    "\n",
    "#     # Create tree from the candidate points\n",
    "#     tree = BallTree(candidates, leaf_size=15, metric='haversine')\n",
    "\n",
    "#     # Find closest points and distances\n",
    "#     distances, indices = tree.query(src_points, k=k_neighbors)\n",
    "\n",
    "#     # Transpose to get distances and indices into arrays\n",
    "#     distances = distances.transpose()\n",
    "#     indices = indices.transpose()\n",
    "\n",
    "#     # Get closest indices and distances (i.e. array at index 0)\n",
    "#     # note: for the second closest points, you would take index 1, etc.\n",
    "#     closest = indices[0]\n",
    "#     closest_dist = distances[0]\n",
    "\n",
    "#     # Return indices and distances\n",
    "#     return (closest, closest_dist)\n",
    "\n",
    "\n",
    "# def nearest_neighbor(left_gdf, right_gdf, return_dist=False):\n",
    "#     \"\"\"\n",
    "#     For each point in left_gdf, find closest point in right GeoDataFrame and return them.\n",
    "\n",
    "#     NOTICE: Assumes that the input Points are in WGS84 projection (lat/lon).\n",
    "#     \"\"\"\n",
    "\n",
    "#     left_geom_col = left_gdf.geometry.name\n",
    "#     right_geom_col = right_gdf.geometry.name\n",
    "\n",
    "#     # Ensure that index in right gdf is formed of sequential numbers\n",
    "#     right = right_gdf.copy().reset_index(drop=True)\n",
    "\n",
    "#     # Parse coordinates from points and insert them into a numpy array as RADIANS\n",
    "#     left_radians = np.array(left_gdf[left_geom_col].apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n",
    "#     right_radians = np.array(right[right_geom_col].apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n",
    "\n",
    "#     # Find the nearest points\n",
    "#     # -----------------------\n",
    "#     # closest ==> index in right_gdf that corresponds to the closest point\n",
    "#     # dist ==> distance between the nearest neighbors (in meters)\n",
    "\n",
    "#     closest, dist = get_nearest(src_points=left_radians, candidates=right_radians)\n",
    "\n",
    "#     # Return points from right GeoDataFrame that are closest to points in left GeoDataFrame\n",
    "#     closest_points = right.loc[closest]\n",
    "\n",
    "#     # Ensure that the index corresponds the one in left_gdf\n",
    "#     closest_points = closest_points.reset_index(drop=True)\n",
    "\n",
    "#     # Add distance if requested\n",
    "#     if return_dist:\n",
    "#         # Convert to meters from radians\n",
    "#         earth_radius = 6371000  # meters\n",
    "#         closest_points['distance'] = dist * earth_radius\n",
    "\n",
    "#     return closest_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find closest station to each station and get also the distance based on haversine distance\n",
    "# # Note: haversine distance which is implemented here is a bit slower than using e.g. 'euclidean' metric\n",
    "# # but useful as we get the distance between points in meters\n",
    "# closest_stops = nearest_neighbor(data2,data2, return_dist=True)\n",
    "\n",
    "# closest_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values identifyied with \"null\"\n",
    "print(100*df1.isnull().sum()/df1.isnull().count())\n",
    "print(\"\")\n",
    "print(\"For the grouped dataset now:\")\n",
    "print(100*df2.isnull().sum()/df2.isnull().count())\n",
    "df2.fillna(df2.ffill(),inplace=True)\n",
    "df1.fillna(df1.ffill(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.fillna(df1.mean(),inplace=True)\n",
    "print(100*df1.isnull().sum()/df1.isnull().count())\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unique values column \n",
    "print({col: df1[col].nunique() for col in df1.columns})\n",
    "df1 = df1.drop(['Red','Province'], axis = 1)\n",
    "#drop unecessary grouped columns\n",
    "df2 = df2.drop(['Longitude','Latitude','dateunix'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummifying Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.get_dummies(df1, columns=['season'], drop_first=False, prefix='season')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance to large cities\n",
    "# vectorized haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2, to_radians=True, earth_radius=6371):\n",
    "    if to_radians:\n",
    "        lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n",
    "\n",
    "        a = np.sin((lat2-lat1)/2.0)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin((lon2-lon1)/2.0)**2\n",
    "        element = earth_radius * 2 * np.arcsin(np.sqrt(a))\n",
    "    return element \n",
    "#la coruna, santiago de compostela and ferrol \n",
    "df1['coruna_l'] = 43.3623\n",
    "df1['coruna_lo'] = -8.396\n",
    "df1['santiago_l'] = 42.8782\n",
    "df1['santiago_lo'] = -8.5448\n",
    "df1['ferrol_l'] = 43.4896\n",
    "df1['ferrol_lo'] = -8.2193\n",
    "df1['Coruna'] =  haversine(df1.loc[:, 'Latitude'], df1.loc[:, 'Longitude'], \n",
    "                             df1.loc[:, 'coruna_l'], df1.loc[:,'coruna_lo'])\n",
    "df1['Santiago'] =  haversine(df1.loc[:, 'Latitude'], df1.loc[:, 'Longitude'], \n",
    "                             df1.loc[:, 'santiago_l'], df1.loc[:,'santiago_lo'])\n",
    "df1['Ferrol'] =  haversine(df1.loc[:, 'Latitude'], df1.loc[:, 'Longitude'], \n",
    "                             df1.loc[:, 'ferrol_l'], df1.loc[:,'ferrol_lo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(['coruna_l','coruna_lo','santiago_l','santiago_lo','ferrol_l','ferrol_lo'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flagedcities(val):\n",
    "    if float(val) <= 1:\n",
    "        val = 1\n",
    "    else:\n",
    "        val = 0\n",
    "    return val  \n",
    "for col in df1.iloc[:,-3:].columns:\n",
    "    df1[col] = np.vectorize(flagedcities)(df1[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"city\"] = np.maximum.reduce(df1[['Coruna', 'Santiago','Ferrol']].values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1[df1.city != 0])/1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(['Coruna','Santiago','Ferrol'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by Service Station "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demand by service station\n",
    "pos_demand = df1.pivot(index=\"Date\", columns=\"Code\", values=\"Gasoline demand\")\n",
    "pos_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price by service station\n",
    "pos_price = df1.pivot(index=\"Date\", columns=\"Code\", values=\"Gasoline price\")\n",
    "pos_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_price =pd.DataFrame(pos_price.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pos_price.loc[:, pos_price.columns != 'Date']\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = pos_price.loc[:, pos_price.columns != 'Date']\n",
    "\n",
    "# poscolnames = []\n",
    "# for col in pos_price.loc[:, pos_price.columns != 'Date']:\n",
    "#     poscolnames.append(col)\n",
    "    \n",
    "# sns.set_palette(\"Paired\",7)\n",
    "\n",
    "# for i in poscolnames:\n",
    "#     sns.lineplot(x=\"Date\", y=i, data=pos, label=i, linewidth = 4.5)\n",
    "\n",
    "# sns.set(rc={'figure.figsize':(20,10)})\n",
    "# sns.despine()\n",
    "# sns.set_style(\"ticks\")\n",
    "\n",
    "\n",
    "# plt.legend(bbox_to_anchor=(1, 0.8), loc='upper left', prop={'size': 16}, title = 'Musical Features', title_fontsize='18')\n",
    "\n",
    "\n",
    "# plt.title('Average Score on Musical Features Time Trend',fontsize=30, weight='bold')\n",
    "# plt.xlabel('Years',fontsize=20, weight='bold')\n",
    "# plt.ylabel('Average Score',fontsize=20, weight='bold')\n",
    "# plt.tick_params(labelsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array of station, respective elasticities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all stations within an array to fit a model for each one of them \n",
    "unique_code = np.unique(df1['Code'])\n",
    "code_arr = []\n",
    "for codes in unique_code:\n",
    "    code_arr.append(df1[df1.Code == codes])\n",
    "code_arr_el = []\n",
    "for stations in range(0,len(code_arr)): \n",
    "    df4 = code_arr[stations].copy()\n",
    "    df4.fillna(df4.ffill(),inplace=True, axis = 1)\n",
    "    df4.loc[(df4['Diesel demand'] == 0) & (df4['Gasoline demand'] == 0), ['Diesel demand','Gasoline demand']] = np.NaN\n",
    "    df4 = df4.dropna()\n",
    "    change_priceD = (((df4['Diesel price']*10)- (df4['Diesel price']*10).shift(1)-1)/ ((df4['Diesel price']*10).shift(1)-1))*100\n",
    "    change_demandD = (((df4['Diesel demand']*10)- (df4['Diesel demand']*10).shift(1)-1)/ ((df4['Diesel demand']*10).shift(1)-1))*100\n",
    "    change_priceG = (((df4['Gasoline price']*10)- (df4['Gasoline price']*10).shift(1)-1)/((df4['Gasoline price']*10).shift(1)-1))*100\n",
    "    change_demandG = (((df4['Gasoline demand']*10)- (df4['Gasoline demand']*10).shift(1)-1)/ ((df4['Gasoline demand']*10).shift(1)-1))*100\n",
    "    df4['Gas elasticity'] = change_demandG/change_priceG;\n",
    "    df4['Diesel elasticity'] = change_demandD/ change_priceD;\n",
    "    print(\"Station \",df4.iloc[1,1], \" \"  ,len(df4) , \" open days\")\n",
    "    code_arr_el.append(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_arr_el[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the elasticity of each station as a dataframe and dictionnary \n",
    "elasticity_stationG = {}\n",
    "elasticity_stationD = {}\n",
    "elasticity_df = pd.DataFrame(columns = {\"Stations\",'Gas elasticity','Diesel elasticity'})\n",
    "for stations in range(0,len(code_arr)):\n",
    "    print(\"Station number\", stations, \"station: \", code_arr_el[stations].iloc[1,1])\n",
    "    print(code_arr_el[stations].iloc[:,4:8].describe())\n",
    "    print('')\n",
    "    elasticity_stationG[code_arr_el[stations].iloc[1,1]] = np.mean(code_arr_el[stations].loc[:,'Gas elasticity'])\n",
    "    elasticity_stationD[code_arr_el[stations].iloc[1,1]] = np.mean(code_arr_el[stations].loc[:,'Diesel elasticity'])\n",
    "    elasticity_df = elasticity_df.append({\"Stations\": code_arr_el[stations].iloc[1,1],\n",
    "                                         \"Gas elasticity\": np.mean(code_arr_el[stations].loc[:,'Gas elasticity']),\n",
    "                                          'Diesel elasticity':np.mean(code_arr_el[stations].loc[:,'Diesel elasticity'])\n",
    "                                         },ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorder station order, observe the elasticity statistics \n",
    "cols = elasticity_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "elasticity_df = elasticity_df[cols] \n",
    "print(elasticity_df)\n",
    "print(elasticity_df.iloc[:,1:3].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.iloc[:,0:28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_arr_scaled = []\n",
    "for stations in range(0,len(code_arr)):\n",
    "    df6 = code_arr_el[stations]\n",
    "    df1_scaled = df6.copy()\n",
    "    df6.drop(['Longitude','Latitude','city'],axis =1)\n",
    "    for col in df6.columns: \n",
    "        if (df1_scaled[col].nunique() > 31) & (col != 'Date') & (col != 'Gasoline demand') & (col != 'Gasoline price') & (col != 'Diesel demand') & (col != 'Diesel price'):\n",
    "            dat = df1_scaled.loc[:,col]\n",
    "            try:\n",
    "                dat = pd.DataFrame(dat.apply(pd.to_numeric))\n",
    "                print(col)\n",
    "            except:\n",
    "                continue\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(dat)\n",
    "            df_scaling = scaler.transform(dat)\n",
    "            df1_scaled.loc[:,col] = df_scaling\n",
    "        else: \n",
    "            continue\n",
    "    code_arr_scaled.append(df1_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_arr_scaled[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:blue;  font-size: 30px; text-align:center;  font-weight: bold;\"> First model  </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blocked Time Series Split\n",
    "class BlockingTimeSeriesSplit():\n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        k_fold_size = n_samples // self.n_splits\n",
    "        indices = np.arange(n_samples)\n",
    "        margin = 0\n",
    "        for i in range(self.n_splits):\n",
    "            start = i * k_fold_size\n",
    "            stop = start + k_fold_size\n",
    "            mid = int(0.8 * (stop - start)) + start\n",
    "            yield indices[start: mid], indices[mid + margin: stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict gasoline demand using global dataset (not considering location)\n",
    "df1_regr = pd.DataFrame()\n",
    "for col in df2.columns:\n",
    "    try:\n",
    "        if (col == \"Date\"):\n",
    "            continue\n",
    "        df1_regr[col] = pd.to_numeric(df2[col], downcast=\"float\")\n",
    "    except:\n",
    "        print(col)\n",
    "        continue\n",
    "X = df1_regr.loc[:, (df1_regr.columns != dd) & (df1_regr.columns != gd)  ]\n",
    "y = df1_regr.loc[:,gd]\n",
    "print(X)\n",
    "score_dict = {}\n",
    "score_dictA = {}\n",
    "mod = [\"Linear Regression \",\"Ridge \",\"Lasso \",\"Bayesian Ridge \"]\n",
    "count = 0\n",
    "for model in [LinearRegression(),Ridge(),Lasso(),BayesianRidge()]:\n",
    "    print('Our model is ' + str(mod[count]))\n",
    "    for i in range(2,10):\n",
    "        btscv = BlockingTimeSeriesSplit(n_splits=i)\n",
    "        print(\"Number of splits: \" + str(i))\n",
    "        for train_index, test_index in btscv.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=btscv, scoring='neg_mean_squared_error')\n",
    "        score_dict[str(i)+ \" \" + str(mod[count])] = scores.mean()\n",
    "        score_dictA[str(i)+ \" \" + str(mod[count])] = abs(scores.mean())\n",
    "        print(\"Loss: {0:.3f} (+/- {1:.3f})\".format(scores.mean(), scores.std()))\n",
    "        print('')\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minval = min(score_dictA.values())\n",
    "result = list(filter(lambda x: score_dictA[x]==minval, score_dictA))\n",
    "result = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result + \"score is: \" + str(score_dict[result]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model importance using ridge \n",
    "count = 0\n",
    "rmse = {}\n",
    "btscv = BlockingTimeSeriesSplit(n_splits=9)\n",
    "for train_index, test_index in btscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=btscv, scoring='neg_mean_squared_error')\n",
    "    # predicting\n",
    "    model_b = Ridge()\n",
    "    model_b.fit(X_train,y_train)\n",
    "    predictions = model_b.predict(X_test)\n",
    "    # evaluating the model with RMSE metric\n",
    "    rmse[\"fold \" + str(count)] = np.sqrt(mean_squared_error(y_test,predictions))\n",
    "    count =+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(clf, feature_names):\n",
    "    return pd.DataFrame({'variable': feature_names, # Feature names\n",
    "                         'coefficient': clf.coef_# Feature Coeficients\n",
    "                    }) \\\n",
    "    .round(decimals=2) \\\n",
    "    .sort_values('coefficient', ascending=False) \\\n",
    "    .style.bar(color=['red', 'green'], align='zero')\n",
    "get_feature_importance(model_b, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the model for the divided df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict gasoline demand using global dataset (not considering location)\n",
    "score_dict = {}\n",
    "score_dictA = {}\n",
    "result_df = pd.DataFrame(columns = {\"Splits\",'model','station','score_mean', 'score_deviation'})\n",
    "for stations in range(0,len(code_arr_scaled)): \n",
    "    df3 = code_arr_scaled[stations]\n",
    "    df1_regr = pd.DataFrame()\n",
    "    for col in df3.columns:\n",
    "        try:\n",
    "            if (col == \"Date\"):\n",
    "                continue\n",
    "            df1_regr[col] = pd.to_numeric(df3[col], downcast=\"float\")\n",
    "        except:\n",
    "            #print(col)\n",
    "            continue\n",
    "    X = df1_regr.loc[:, (df1_regr.columns != dd) & (df1_regr.columns != gd) & (df1_regr.columns != 'Diesel elasticity') & (df1_regr.columns != 'Gas elasticity')]\n",
    "    y = df1_regr.loc[:,gd]\n",
    "    mod = [\"Linear Regression \",\"Ridge \",\"Lasso \",\"Bayesian Ridge \"]\n",
    "    count = 0\n",
    "    for model in [LinearRegression(),Ridge(normalize=True),Lasso(),BayesianRidge()]:\n",
    "        #print('Our model is ' + str(mod[count]))\n",
    "        for i in range(2,10):\n",
    "            btscv = BlockingTimeSeriesSplit(n_splits=i)\n",
    "            #print(\"Number of splits: \" + str(i))\n",
    "            for train_index, test_index in btscv.split(X):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=btscv, scoring='neg_mean_squared_error')\n",
    "            score_dict[str(i)+ \"/ \" + str(mod[count]) + \"/ station code/\" + str(stations)] = scores.mean()\n",
    "            score_dictA[str(i)+ \"/ \" + str(mod[count]) + \"/ station code/\" + str(stations)] = abs(scores.mean())\n",
    "            result_df = result_df.append({\"Splits\": i,\n",
    "                                         \"model\": mod[count],\n",
    "                                         'station':df3.iloc[1,1],\n",
    "                                          \"score_mean\": abs(scores.mean()),\n",
    "                                          'score_deviation': scores.std()\n",
    "                                         },ignore_index=True) \n",
    "            #print(\"Loss: {0:.3f} (+/- {1:.3f})\".format(scores.mean(), scores.std()))\n",
    "            #print('')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.groupby(\"station\").agg({\"score_mean\":\"min\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model importance using ridge \n",
    "count = 0\n",
    "rmse = {}\n",
    "btscv = BlockingTimeSeriesSplit(n_splits=9)\n",
    "for train_index, test_index in btscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=btscv, scoring='neg_mean_squared_error')\n",
    "    # predicting\n",
    "    model_b = Ridge()\n",
    "    model_b.fit(X_train,y_train)\n",
    "    predictions = model_b.predict(X_test)\n",
    "    # evaluating the model with RMSE metric\n",
    "    rmse[\"fold \" + str(count)] = np.sqrt(mean_squared_error(y_test,predictions))\n",
    "    count =+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(clf, feature_names):\n",
    "    return pd.DataFrame({'variable': feature_names, # Feature names\n",
    "                         'coefficient': clf.coef_# Feature Coeficients\n",
    "                    }) \\\n",
    "    .round(decimals=2) \\\n",
    "    .sort_values('coefficient', ascending=False) \\\n",
    "    .style.bar(color=['red', 'green'], align='zero')\n",
    "get_feature_importance(model_b, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
